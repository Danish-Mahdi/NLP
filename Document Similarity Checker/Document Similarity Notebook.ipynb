{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "addf2f1b",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Document similarity measures how alike two or more texts are based on their content, which is essential for applications like information retrieval, clustering, and recommendation systems. In this notebook, we will utilize **Cosine Similarity** as our primary method for evaluating this similarity of docs in the dataset of 20NewsGroups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c9493d",
   "metadata": {},
   "source": [
    "# Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0e6004",
   "metadata": {},
   "source": [
    "## 1. Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ed61a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_numeric, strip_punctuation, strip_multiple_whitespaces\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6aa1c3",
   "metadata": {},
   "source": [
    "## 2. Load dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50d247db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_data = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "print(newsgroups_data.target_names)  # List of 20 categories\n",
    "# print(newsgroups_data.data[0])       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a440db",
   "metadata": {},
   "source": [
    "## 3. Creating a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6802234b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>category_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nI am sure some bashers of Pens fans are pr...</td>\n",
       "      <td>10</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My brother is in the market for a high-perform...</td>\n",
       "      <td>3</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n\\n\\n\\tFinally you said what you dream abou...</td>\n",
       "      <td>17</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nThink!\\n\\nIt's the SCSI card doing the DMA t...</td>\n",
       "      <td>3</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1)    I have an old Jasmine drive which I cann...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  category  \\\n",
       "0  \\n\\nI am sure some bashers of Pens fans are pr...        10   \n",
       "1  My brother is in the market for a high-perform...         3   \n",
       "2  \\n\\n\\n\\n\\tFinally you said what you dream abou...        17   \n",
       "3  \\nThink!\\n\\nIt's the SCSI card doing the DMA t...         3   \n",
       "4  1)    I have an old Jasmine drive which I cann...         4   \n",
       "\n",
       "              category_name  \n",
       "0          rec.sport.hockey  \n",
       "1  comp.sys.ibm.pc.hardware  \n",
       "2     talk.politics.mideast  \n",
       "3  comp.sys.ibm.pc.hardware  \n",
       "4     comp.sys.mac.hardware  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Create DataFrame\n",
    "data_df = pd.DataFrame({\n",
    "    'text': newsgroups_data.data,      \n",
    "    'category': newsgroups_data.target  \n",
    "})\n",
    "# Map category integers to their respective names \n",
    "data_df['category_name'] = data_df['category'].apply(lambda x: newsgroups_data.target_names[x])\n",
    "\n",
    "# Show the first few rows\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3de1b9e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nI am sure some bashers of Pens fans are pretty confused about the lack\\nof any kind of posts about the recent Pens massacre of the Devils. Actually,\\nI am  bit puzzled too and a bit relieved. However, I am going to put an end\\nto non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\\nare killing those Devils worse than I thought. Jagr just showed you why\\nhe is much better than his regular season stats. He is also a lot\\nfo fun to watch in the playoffs. Bowman should let JAgr have a lot of\\nfun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\\nregular season game.          PENS RULE!!!\\n\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4329e8",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning & Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3544861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    filters = [\n",
    "        lambda x: x.lower(),           \n",
    "        strip_tags,                    # Remove HTML tags \n",
    "        strip_numeric,                 \n",
    "        strip_punctuation,             \n",
    "        strip_multiple_whitespaces     \n",
    "    ]\n",
    "    text = ' '.join(preprocess_string(text, filters=filters))\n",
    "    \n",
    "    # NLTK preprocessing\n",
    "    words = word_tokenize(text)  \n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "  \n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in lemmatized_words if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply preprocessing steps to the 'text' column\n",
    "data_df['text_Preprocessed'] = data_df['text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9da9e9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop original text column\n",
    "data_df=data_df.drop('text', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abd89d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>category_name</th>\n",
       "      <th>text_Preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>sure bashers pen fan pretty confused lack kind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "      <td>brother market high performance video card sup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>finally said dream mediterranean wa new area g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "      <td>think scsi card dma transfer disk scsi card dm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>old jasmine drive use new system understanding...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category             category_name  \\\n",
       "0        10          rec.sport.hockey   \n",
       "1         3  comp.sys.ibm.pc.hardware   \n",
       "2        17     talk.politics.mideast   \n",
       "3         3  comp.sys.ibm.pc.hardware   \n",
       "4         4     comp.sys.mac.hardware   \n",
       "\n",
       "                                   text_Preprocessed  \n",
       "0  sure bashers pen fan pretty confused lack kind...  \n",
       "1  brother market high performance video card sup...  \n",
       "2  finally said dream mediterranean wa new area g...  \n",
       "3  think scsi card dma transfer disk scsi card dm...  \n",
       "4  old jasmine drive use new system understanding...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "216cd515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['category', 'category_name', 'text_Preprocessed'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9626b7c0",
   "metadata": {},
   "source": [
    "## 5. Feature Extraction using Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed606fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    aa  aaron   ab  abc  abiding   ability  able  abortion  absence  absolute  \\\n",
      "0  0.0    0.0  0.0  0.0      0.0  0.000000   0.0       0.0      0.0       0.0   \n",
      "1  0.0    0.0  0.0  0.0      0.0  0.000000   0.0       0.0      0.0       0.0   \n",
      "2  0.0    0.0  0.0  0.0      0.0  0.000000   0.0       0.0      0.0       0.0   \n",
      "3  0.0    0.0  0.0  0.0      0.0  0.067664   0.0       0.0      0.0       0.0   \n",
      "4  0.0    0.0  0.0  0.0      0.0  0.000000   0.0       0.0      0.0       0.0   \n",
      "\n",
      "   ...   ze  zealand  zei  zero  zionism  zionist  zip  zone  zoom   zx  \n",
      "0  ...  0.0      0.0  0.0   0.0      0.0      0.0  0.0   0.0   0.0  0.0  \n",
      "1  ...  0.0      0.0  0.0   0.0      0.0      0.0  0.0   0.0   0.0  0.0  \n",
      "2  ...  0.0      0.0  0.0   0.0      0.0      0.0  0.0   0.0   0.0  0.0  \n",
      "3  ...  0.0      0.0  0.0   0.0      0.0      0.0  0.0   0.0   0.0  0.0  \n",
      "4  ...  0.0      0.0  0.0   0.0      0.0      0.0  0.0   0.0   0.0  0.0  \n",
      "\n",
      "[5 rows x 5000 columns]\n"
     ]
    }
   ],
   "source": [
    "vectorizer=TfidfVectorizer(max_features=5000, ngram_range=(1,1))\n",
    "tfidf_matrix = vectorizer.fit_transform(data_df['text_Preprocessed'])\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(tfidf_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdfc2e0",
   "metadata": {},
   "source": [
    "## 6. Calculating Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e38382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "cosine_sim_df = pd.DataFrame(cosine_sim, columns=data_df.index, index=data_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aec31ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>18836</th>\n",
       "      <th>18837</th>\n",
       "      <th>18838</th>\n",
       "      <th>18839</th>\n",
       "      <th>18840</th>\n",
       "      <th>18841</th>\n",
       "      <th>18842</th>\n",
       "      <th>18843</th>\n",
       "      <th>18844</th>\n",
       "      <th>18845</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007907</td>\n",
       "      <td>0.052631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010961</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063080</td>\n",
       "      <td>0.007643</td>\n",
       "      <td>0.002370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053269</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021328</td>\n",
       "      <td>0.004695</td>\n",
       "      <td>0.034655</td>\n",
       "      <td>0.017693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011421</td>\n",
       "      <td>0.006884</td>\n",
       "      <td>0.019153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007907</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150919</td>\n",
       "      <td>0.034603</td>\n",
       "      <td>0.031700</td>\n",
       "      <td>0.031316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019442</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008431</td>\n",
       "      <td>0.013821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.052631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003837</td>\n",
       "      <td>0.014205</td>\n",
       "      <td>0.002583</td>\n",
       "      <td>0.049734</td>\n",
       "      <td>0.007678</td>\n",
       "      <td>0.041863</td>\n",
       "      <td>0.068258</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084711</td>\n",
       "      <td>0.019511</td>\n",
       "      <td>0.065632</td>\n",
       "      <td>0.041922</td>\n",
       "      <td>0.036146</td>\n",
       "      <td>0.029691</td>\n",
       "      <td>0.017310</td>\n",
       "      <td>0.002729</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150919</td>\n",
       "      <td>0.003837</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.055950</td>\n",
       "      <td>0.004544</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004021</td>\n",
       "      <td>0.007265</td>\n",
       "      <td>0.005010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002342</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008647</td>\n",
       "      <td>0.013568</td>\n",
       "      <td>0.005027</td>\n",
       "      <td>0.003677</td>\n",
       "      <td>0.006469</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034603</td>\n",
       "      <td>0.014205</td>\n",
       "      <td>0.055950</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.020244</td>\n",
       "      <td>0.043863</td>\n",
       "      <td>0.007341</td>\n",
       "      <td>0.018427</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019069</td>\n",
       "      <td>0.014163</td>\n",
       "      <td>0.020090</td>\n",
       "      <td>0.030402</td>\n",
       "      <td>0.008214</td>\n",
       "      <td>0.013722</td>\n",
       "      <td>0.010422</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 18846 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0         1         2         3         4         5         6      \\\n",
       "0  1.000000  0.007907  0.052631  0.000000  0.000000  0.010961  0.000000   \n",
       "1  0.007907  1.000000  0.000000  0.150919  0.034603  0.031700  0.031316   \n",
       "2  0.052631  0.000000  1.000000  0.003837  0.014205  0.002583  0.049734   \n",
       "3  0.000000  0.150919  0.003837  1.000000  0.055950  0.004544  0.000000   \n",
       "4  0.000000  0.034603  0.014205  0.055950  1.000000  0.020244  0.043863   \n",
       "\n",
       "      7         8         9      ...     18836     18837     18838     18839  \\\n",
       "0  0.063080  0.007643  0.002370  ...  0.053269  0.000000  0.021328  0.004695   \n",
       "1  0.000000  0.008909  0.000000  ...  0.019442  0.000000  0.008431  0.013821   \n",
       "2  0.007678  0.041863  0.068258  ...  0.084711  0.019511  0.065632  0.041922   \n",
       "3  0.004021  0.007265  0.005010  ...  0.002342  0.000000  0.008647  0.013568   \n",
       "4  0.007341  0.018427  0.000000  ...  0.000000  0.019069  0.014163  0.020090   \n",
       "\n",
       "      18840     18841     18842     18843     18844     18845  \n",
       "0  0.034655  0.017693  0.000000  0.011421  0.006884  0.019153  \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.050957  \n",
       "2  0.036146  0.029691  0.017310  0.002729  0.000000  0.023256  \n",
       "3  0.005027  0.003677  0.006469  0.000000  0.000000  0.021086  \n",
       "4  0.030402  0.008214  0.013722  0.010422  0.000000  0.073181  \n",
       "\n",
       "[5 rows x 18846 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea5c777",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this notebook, we used Cosine Similarity to assess document relationships, with scores ranging from -1 to +1. A score of 1 indicates identical documents, 0 means no similarity, and -1 suggests opposite content. This analysis highlighted clusters of similar texts and potential duplicates, showcasing the effectiveness of cosine similarity in enhancing text analysis and information retrieval.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ebccda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
